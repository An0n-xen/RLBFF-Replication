{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f51ec24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "from itertools import islice\n",
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f274a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, input_file=\"\"):\n",
    "        self.input_file = input_file\n",
    "        self.output_file = \"final_training_data.jsonl\"\n",
    "        \n",
    "        # Loading a small, fast embedding model for consensus check\n",
    "        print(\"Loading Sentence Transformer model...\")\n",
    "        # self.encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        self.encoder = SentenceTransformer(\"Qwen/Qwen3-Embedding-8B\")\n",
    "        \n",
    "    def _parse_json_safely(self, json_str):\n",
    "        \"\"\"\n",
    "        Robustly parses JSON even if the LLM wraps it in markdown blocks\n",
    "        or adds extra text.\n",
    "        \"\"\"\n",
    "        if not json_str:\n",
    "            return None\n",
    "\n",
    "        # 1. Strip Markdown Code Blocks (```json ... ```)\n",
    "        # This regex looks for the content inside ``` ... ``` tags\n",
    "        match = re.search(r\"```(?:json)?\\s*(.*)\\s*```\", json_str, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)\n",
    "        \n",
    "        # 2. Try parsing cleaned string\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            pass # Parsing failed, try the backup method below\n",
    "\n",
    "        # 3. BACKUP: Regex extraction (If LLM messed up commas/quotes)\n",
    "        # Sometimes LLMs forget the last brace '}' or add text after it.\n",
    "        try:\n",
    "            # Find the first '{' and the last '}'\n",
    "            start = json_str.find(\"{\")\n",
    "            end = json_str.rfind(\"}\")\n",
    "            if start != -1 and end != -1:\n",
    "                clean_str = json_str[start : end + 1]\n",
    "                return json.loads(clean_str)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        print(f\"Warning: Could not parse JSON: {json_str[:50]}...\")\n",
    "        return None\n",
    "        \n",
    "    def filter_step_1_and_2(self, raw_data):\n",
    "        \"\"\"\n",
    "        Filter 1: Hallucination Check (RapidFuzz > 60)\n",
    "        Filter 2: Ambiguity Check (Remove 'partially')\n",
    "        \"\"\"\n",
    "        \n",
    "        clean_candidates = []\n",
    "        \n",
    "        for entry in raw_data:\n",
    "            feedback_text = entry[\"original_feedback\"]\n",
    "            raw_principles = self._parse_json_safely(entry[\"extracted_json\"])\n",
    "            \n",
    "            if isinstance(feedback_text, list):\n",
    "                feedback_text = max(feedback_text, key=len)\n",
    "            \n",
    "            if not raw_principles:\n",
    "                continue\n",
    "            \n",
    "            valid_principles_for_this_entry = []\n",
    "            \n",
    "            for principle_name, value in raw_principles.items():\n",
    "                if \"-\" not in value:\n",
    "                    continue\n",
    "                \n",
    "                # Split evidence from label\n",
    "                parts = value.rsplit(\"-\", 1)\n",
    "                evidence_span = parts[0].strip()\n",
    "                label = parts[1].strip().lower()\n",
    "                \n",
    "                \n",
    "                # --- FILTER 2: Ambiguity ---\n",
    "                if \"partially\" in label:\n",
    "                    continue # Skip partials\n",
    "                \n",
    "                # Normalize label to just Yes/No\n",
    "                if \"yes\" in label: final_label = \"Yes\"\n",
    "                elif \"no\" in label: final_label = \"No\"\n",
    "                else: continue\n",
    "                \n",
    "                \n",
    "                # --- FILTER 1: Hallucination ---\n",
    "                # Check if evidence actually exists in text\n",
    "                match_score = fuzz.partial_ratio(evidence_span.lower(), feedback_text.lower())\n",
    "                if match_score <= 60: # Threshold from paper \n",
    "                    continue\n",
    "                \n",
    "                valid_principles_for_this_entry.append({\n",
    "                    \"principle\": principle_name,\n",
    "                    \"label\": final_label,\n",
    "                    \"evidence\": evidence_span\n",
    "                })\n",
    "                \n",
    "            if valid_principles_for_this_entry:\n",
    "                clean_candidates.append({\n",
    "                    \"prompt_id\": entry['prompt_id'],\n",
    "                    \"principles\": valid_principles_for_this_entry\n",
    "                })\n",
    "                \n",
    "                \n",
    "        print(f\"Step 1 & 2 Complete. {len(clean_candidates)} valid entries kept.\")\n",
    "        return clean_candidates\n",
    "    \n",
    "    def filter_step_3(self, candidates):\n",
    "        \"\"\"\n",
    "        Filter 3: Consensus Check\n",
    "        \"\"\"\n",
    "        from itertools import islice # Ensure this is imported if you ever use it again\n",
    "\n",
    "        # Group by prompt_id\n",
    "        grouped = defaultdict(list)\n",
    "        for c in candidates:\n",
    "            grouped[c['prompt_id']].append(c['principles'])\n",
    "            \n",
    "        final_dataset = []\n",
    "        \n",
    "        for p_id, annotator_lists in grouped.items(): \n",
    "            \n",
    "            if len(annotator_lists) < 2:\n",
    "                # If you trust single annotators, add them directly:\n",
    "                # print(f\"Prompt {p_id} has 1 annotator. Keeping without consensus check.\")\n",
    "                \n",
    "                # Flatten the list structure since it's a list of lists\n",
    "                single_annotator_principles = annotator_lists[0] \n",
    "                \n",
    "                final_dataset.append({\n",
    "                    \"prompt_id\": p_id,\n",
    "                    \"verified_principles\": single_annotator_principles\n",
    "                })\n",
    "                continue \n",
    "\n",
    "            # (This runs only if there are 2+ annotators)\n",
    "            for i, current_list in enumerate(annotator_lists):\n",
    "                other_principles = []\n",
    "                for j, other_list in enumerate(annotator_lists):\n",
    "                    if i == j: continue\n",
    "                    other_principles.extend([p['principle'] for p in other_list])\n",
    "                \n",
    "                if not other_principles: continue\n",
    "\n",
    "                current_texts = [p['principle'] for p in current_list]\n",
    "                curr_embeddings = self.encoder.encode(current_texts)\n",
    "                other_embeddings = self.encoder.encode(other_principles)\n",
    "\n",
    "                similarity_matrix = cosine_similarity(curr_embeddings, other_embeddings)\n",
    "                \n",
    "                valid_principles = []\n",
    "                for k, p_obj in enumerate(current_list):\n",
    "                    if similarity_matrix[k].max() > 0.8:\n",
    "                        valid_principles.append(p_obj)\n",
    "                \n",
    "                if valid_principles:\n",
    "                    final_dataset.append({\n",
    "                        \"prompt_id\": p_id,\n",
    "                        \"verified_principles\": valid_principles\n",
    "                    })\n",
    "                    \n",
    "        print(f\"Step 3 Complete. {len(final_dataset)} verified entries.\")\n",
    "        return final_dataset\n",
    "    \n",
    "    def run(self):\n",
    "        # 1. Load Raw Data\n",
    "        print(\"Loading raw data...\")\n",
    "        raw_data = []\n",
    "        with open(self.input_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                raw_data.append(json.loads(line))\n",
    "    \n",
    "        # 2. Run Filters 1 & 2\n",
    "        candidates = self.filter_step_1_and_2(raw_data)\n",
    "        \n",
    "        # 3. Run Filter 3\n",
    "        final_data = self.filter_step_3(candidates)\n",
    "        \n",
    "        with open(self.output_file, \"w\") as f:\n",
    "            for entry in final_data:\n",
    "                f.write(json.dumps(entry) + \"\\n\")\n",
    "        print(f\"Done! Final dataset saved to {self.output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "066b6210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [16:15<00:00, 243.78s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:17<00:00, 19.41s/it]\n"
     ]
    }
   ],
   "source": [
    "cleaner = DataCleaner(input_file=\"./data/extracted_principles.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e29eba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data...\n",
      "Warning: Could not parse JSON: {\n",
      "  \"Accuracy of Field Terminator\": \"The response ...\n",
      "Warning: Could not parse JSON: {\n",
      "  \"Correctness of Code\": \"However, the code seem...\n",
      "Step 1 & 2 Complete. 34194 valid entries kept.\n",
      "Step 3 Complete. 34194 verified entries.\n",
      "Done! Final dataset saved to final_training_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "cleaner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "968357b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/extracted_principles.jsonl\"\n",
    "\n",
    "with open(data_path, \"r\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db8973fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "principles = [x['extracted_json'] for x in raw_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e627d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
